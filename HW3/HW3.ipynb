{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pletnev Aleksandr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import randint\n",
    "from nltk.lm.models import Laplace\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import json\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement a function that generates a dinosaur name of random length. The generated dinosaur name should start with the begin symbol < and with the end symbol >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['<' + name.strip().lower() + '>' for name in open('dinos.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Compute frequency of each character\n",
    "chars = [char  for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_symbol = '<'\n",
    "name = \"<\"\n",
    "# Stop generating names when we predict final symbol\n",
    "for x in  list(range(1,100)):\n",
    "    pred_symbol = cprob[pred_symbol].generate()\n",
    "    name+=pred_symbol\n",
    "    if (pred_symbol == '>'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name:  <donyatoruaeliauirurusangocoriloibos>\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated name: \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a a) = 0.0044\n",
      "p(a b) = 0.0097\n",
      "p(a u) = 0.3181\n"
     ]
    }
   ],
   "source": [
    "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement and apply add-one smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = nltk.FreqDist(flatten(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Laplace (add-one) smothing\n",
    "cprob_laplace = nltk.ConditionalProbDist(cfreq, nltk.LaplaceProbDist, bins=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated name:  <eroiauranglolitomonaong>\n"
     ]
    }
   ],
   "source": [
    "pred_symbol = '<'\n",
    "name = \"<\"\n",
    "# Stop generating names when we predict final symbol\n",
    "for x in  list(range(1,100)):\n",
    "    pred_symbol = cprob_laplace[pred_symbol].generate()\n",
    "    name+=pred_symbol\n",
    "    if (pred_symbol == '>'):\n",
    "        break\n",
    "print(\"Generated name: \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a a) = 0.0048\n",
      "p(a b) = 0.0099\n",
      "p(a u) = 0.3149\n"
     ]
    }
   ],
   "source": [
    "print('p(a a) = %1.4f' %cprob_laplace['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob_laplace['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob_laplace['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(y y) = 0.0000\n",
      "p(y y) after smoothing = 0.0034\n"
     ]
    }
   ],
   "source": [
    "print('p(y y) = %1.4f' %cprob['y'].prob('y'))\n",
    "print('p(y y) after smoothing = %1.4f' %cprob_laplace['y'].prob('y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How add one smoothing affects the model and the generated names?\n",
    "- It is hard notice the trend of how it affects the generated names. But it is clearly, that now rare seldom bigrams occur more often and frequent bigram occur less often\n",
    "- If some bigram had 0 probability, now it have at least prob -> names can be generated more variously. On other hand, it allows names to include some unreal combinations of words, for example: yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        #One hidden dense layer\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # Output layer\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        #There are as many units in the output layer, as there are symbols in the dictionar\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        \n",
    "        # Softmax performs terrible (it just stucks) . LogSoftMax is much better\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "#print(h_prev.shape)\n",
    " #       print(x.shape)\n",
    "        input_combined = torch.cat([h_prev, x], dim = 1)\n",
    "        hidden = torch.tanh(self.dropout(self.i2h(input_combined)))\n",
    "        \n",
    "        output = self.i2o(input_combined)\n",
    "        \n",
    "        \n",
    "        #â„–output_combined = torch.cat((hidden, output), 1)\n",
    "        #output = self.o2o(output_combined)\n",
    "        #output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content)) + ['<', '>']\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        #teacher forcing\n",
    "        x_str = '<' + line \n",
    "        y_str = line + '>' \n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end ='') for x in sample_idxs]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_nlm(model, loss_fn, optimizer,dataset = trn_dl):\n",
    "    model.train()\n",
    "    # X - correct input - onehot\n",
    "    # y - onehot\n",
    "    for line_num, (x, y) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = trn_ds.ch_to_idx['<']\n",
    "        indices = [start_char_idx]\n",
    "\n",
    "        x[0, start_char_idx] = 1\n",
    "        word_size=0\n",
    "        predicted_char_idx = start_char_idx\n",
    "        # As in instructions - step read k characters, predict next one\n",
    "        for i in range(1,y.shape[1]):\n",
    "            ## \n",
    "            #THE INPUT OF THE MODEL ARE CONCATENATED ONE-HOT ENCODINGS OF CHARACTERS\n",
    "            # MaX - 870 = 29 * 30 (max length of word)\n",
    "            \n",
    "            # Generate required tensor\n",
    "            xxx = torch.zeros([len(indices), trn_ds.vocab_size], dtype=torch.float)\n",
    "            for idx in range(0,len(indices)):\n",
    "                xxx[idx][indices[idx]] = 1\n",
    "            #THE INPUT OF THE MODEL ARE CONCATENATED ONE-HOT ENCODINGS OF CHARACTERS\n",
    "            xx = torch.cat([xxx[:word_size+1].view(-1), torch.zeros( 870-xxx[:word_size+1].view(-1).shape[0] )], 0)\n",
    "            h_prev, y_pred = model(h_prev, xx[None])\n",
    "                        # as in instructions - softmax\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.detach().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            word_size += 1\n",
    "        if (line_num+1) % 100 == 0:\n",
    "            # Create test output\n",
    "            print_sample(sample_nlm(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nlm(model, loss_fn, optimizer, dataset=trn_dl, epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch_nlm(model, loss_fn, optimizer,dataset)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nlm(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    newline_idx = trn_ds.ch_to_idx['>']\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = trn_ds.ch_to_idx['<']\n",
    "        indices = [start_char_idx]\n",
    "\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        # As in instructios - input - onehot encoded, predict next symbol!\n",
    "        while predicted_char_idx != newline_idx and word_size != 29:\n",
    "            #print(sum(x[:, :(word_size+1)]).shape)\n",
    "            \n",
    "            # Generate required tensor\n",
    "            x = torch.zeros([len(indices), trn_ds.vocab_size], dtype=torch.float)\n",
    "            for idx in range(0,len(indices)):\n",
    "                x[idx][indices[idx]] = 1\n",
    "\n",
    "            #THE INPUT OF THE MODEL ARE CONCATENATED ONE-HOT ENCODINGS OF CHARACTERS\n",
    "            xx = torch.cat([x[:word_size+1].view(-1), torch.zeros( 870-x[:word_size+1].view(-1).shape[0] )], 0)\n",
    "\n",
    "            h_prev, y_pred = model(h_prev, xx[None])\n",
    "            \n",
    "            \n",
    "            # as in instructions - softmax\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.detach().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    "            predicted_char_idx = idx\n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 29:\n",
    "            indices.append(newline_idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 150 #50 #1000 #100\n",
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlm = NLM(trn_ds.vocab_size*30, hidden_size, trn_ds.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(nlm.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<uoareuooaxar>\n",
      "<neulnauas>\n",
      "<dtilauar>\n",
      "<l<totu>\n",
      "<errl<tnrtuaooaluaum>\n",
      "<ilahsasn>\n",
      "<alwpucuuusr>\n",
      "<smerrn>\n",
      "<asssoitus>\n",
      "<rstanoaoue>\n",
      "<gsarhtacsrasn>\n",
      "<snandunpauas>\n",
      "<smanetnoauas>\n",
      "<uailahsaur>\n",
      "<hasalvrydu>\n",
      "\n",
      "Epoch:2\n",
      "<lerrmyusux>\n",
      "<shtaasoargursa>\n",
      "<iwouatuusr>\n",
      "<rrlysnrsuarsas>\n",
      "<iysmpsuarsas>\n",
      "<wouasttrouuu>\n",
      "<lamdsilauas>\n",
      "<atiiataq>\n",
      "<acrnanauosa>\n",
      "<aritacsoaso>\n",
      "<sngrro>\n",
      "<arsrnestr>\n",
      "<rrtaknamuaxr>\n",
      "<kancsinaucs>\n",
      "<trnersn>\n",
      "\n",
      "Epoch:3\n",
      "<uslerso>\n",
      "<ouastusouuu>\n",
      "<uaasoareussa>\n",
      "<inaisauouar>\n",
      "<qrtanoanua>\n",
      "<ldsiiatar>\n",
      "<otasssohtus>\n",
      "<gtacroarlursg>\n",
      "<uaimahsauo>\n",
      "<taluouauuuss>\n",
      "<sariuaausaus>\n",
      "<esariuaausaus>\n",
      "<irrtanoaoue>\n",
      "<igrrnytruu>\n",
      "<yslortaorar>\n",
      "\n",
      "Epoch:4\n",
      "<rtalnagtaur>\n",
      "<uaasnaoaursd>\n",
      "<iyslpsuarsas>\n",
      "<unucsssoiuus>\n",
      "<iysnrsuarsas>\n",
      "<eiasan>\n",
      "<irstanrar>\n",
      "<rrkysosu>\n",
      "<uaknaiuaur>\n",
      "<asakuouauuuts>\n",
      "<andsiiauas>\n",
      "<iamdsilauas>\n",
      "<kanesilauas>\n",
      "<rrkysnsuyasses>\n",
      "<iesiiatar>\n",
      "\n",
      "Epoch:5\n",
      "<iahsasnuanusaur>\n",
      "<iysnpstarsas>\n",
      "<rtalnahtaur>\n",
      "<acrnaobuora>\n",
      "<ashiasao>\n",
      "<uacsoareurse>\n",
      "<rmerro>\n",
      "<allahsaup>\n",
      "<rnersnzuruu>\n",
      "<hkasanusun>\n",
      "<esasiuaautaus>\n",
      "<iprtaloanuc>\n",
      "<ysmrstarsas>\n",
      "<trlerso>\n",
      "<yslqrtaoras>\n",
      "\n",
      "Epoch:6\n",
      "<acrnanduora>\n",
      "<ivouasuurnuus>\n",
      "<irstanoao>\n",
      "<esasiuaautaus>\n",
      "<rmeqplytosu>\n",
      "<rnepplytptu>\n",
      "<smprtanoap>\n",
      "<etacsoargursb>\n",
      "<ilaisasluao>\n",
      "<argtaasrasn>\n",
      "<ritaasoasn<rra>\n",
      "<oubsssrmuut>\n",
      "<aisasnuai>\n",
      "<trmgrso>\n",
      "<hiasan>\n",
      "\n",
      "Epoch:7\n",
      "<untcstsnhtur>\n",
      "<ivoubsturou>\n",
      "<iysnrsuarsas>\n",
      "<argtaasraso>\n",
      "<rmysnpsuarsds>\n",
      "<aisasiuadusaus>\n",
      "<rmostanrar>\n",
      "<taimahsaur>\n",
      "<rtamnaiuaus>\n",
      "<cpplysnsu>\n",
      "<andshiauar>\n",
      "<ietilauar>\n",
      "<aluotasuusou>\n",
      "<riuacsoaso>\n",
      "<esasiuaausass>\n",
      "\n",
      "Epoch:8\n",
      "<arnanetnpaxes>\n",
      "<ivotbsturou>\n",
      "<rtamoaiuaur>\n",
      "<astsnestr>\n",
      "<rtamnamuaus>\n",
      "<errnxsosu>\n",
      "<hatalurueuy>\n",
      "<eiasao>\n",
      "<errnytruu>\n",
      "<etacsoarmussg>\n",
      "<sihasan>\n",
      "<arnanctloa>\n",
      "<arhtaasraro>\n",
      "<erashuaauta>\n",
      "<siiasaoys>\n",
      "\n",
      "Epoch:9\n",
      "<ritadsoasn>\n",
      "<siiasao>\n",
      "<uajnahsaur>\n",
      "<rrtanoanucus>\n",
      "<etacsoariusse>\n",
      "<smpstanrap>\n",
      "<tnostanrar>\n",
      "<oubsssodtus>\n",
      "<allahsaur>\n",
      "<irstamnanud>\n",
      "<tnortanrar>\n",
      "<allahsauoyao>\n",
      "<alvouatuusr>\n",
      "<uaarnaoeurse>\n",
      "<ucrssogtus>\n",
      "\n",
      "Epoch:10\n",
      "<trmgrro>\n",
      "<trmerso>\n",
      "<allaisaur>\n",
      "<uoucsttrnuvt>\n",
      "<ucsssoeuus>\n",
      "<rrlysosuuassis>\n",
      "<iysnpsuaprar>\n",
      "<ortaloalua>\n",
      "<rriysosuuassgs>\n",
      "<kamdsikauas>\n",
      "<aneshiauar>\n",
      "<allagsauo>\n",
      "<amesiiauar>\n",
      "<ivoucsuurou>\n",
      "<rashtaatratp>\n",
      "\n",
      "Epoch:11\n",
      "<rlysnrsuasscs>\n",
      "<siiasan>\n",
      "<armandtloa>\n",
      "<astsnestr>\n",
      "<aretaasraso>\n",
      "<arlanetnoa>\n",
      "<sihasanvsyeu>\n",
      "<rnerrn>\n",
      "<prtamnalua>\n",
      "<jesiiatap>\n",
      "<trmerso>\n",
      "<acroaoaurra>\n",
      "<ivntasturou>\n",
      "<uabsnaodurse>\n",
      "<yslpstapras>\n",
      "\n",
      "Epoch:12\n",
      "<errlytosu>\n",
      "<ysmrsuarsas>\n",
      "<iysnpsuarsas>\n",
      "<uoucsttrou>\n",
      "<allaisaur>\n",
      "<uainahsaur>\n",
      "<iahsashuai>\n",
      "<iprtalnaoud>\n",
      "<rhuacsparn>\n",
      "<rnandtkmayas>\n",
      "<iesiiauar>\n",
      "<iesiiauap>\n",
      "<iakdshiauas>\n",
      "<asamuouauuxsr>\n",
      "<tainahsaup>\n",
      "\n",
      "Epoch:13\n",
      "<artsnhstr>\n",
      "<iwouasttrou>\n",
      "<rlysnpsuarsds>\n",
      "<iagsasiuajutaus>\n",
      "<untcsssrnu>\n",
      "<ortaknaluaus>\n",
      "<ucsssohtts>\n",
      "<armanaumpa>\n",
      "<rnaneslnauas>\n",
      "<rnancsimauas>\n",
      "<iesiiatar>\n",
      "<iagsasiualusaus>\n",
      "<ahsasiuahusaus>\n",
      "<oucsssoiuus>\n",
      "<esarhuaausaus>\n",
      "\n",
      "Epoch:14\n",
      "<ancsiiauas>\n",
      "<rlysnrsuarscs>\n",
      "<idsiiatao>\n",
      "<riuaasraso>\n",
      "<rnerrnyusu>\n",
      "<hasanusuhu>\n",
      "<prlysortuassds>\n",
      "<ialeslnauas>\n",
      "<eiasanvs>\n",
      "<agsasiuaeusaus>\n",
      "<etadsoarkurse>\n",
      "<agrasiuaeusays>\n",
      "<rmerro>\n",
      "<rsrngsso>\n",
      "<sajuotatuusr>\n",
      "\n",
      "Epoch:15\n",
      "<etacsoaphurse>\n",
      "<uotcsssriuus>\n",
      "<raritaatsasr>\n",
      "<rrlysosuxassds>\n",
      "<etacsoapiurse>\n",
      "<acsoaocuora>\n",
      "<ouastsoeuus>\n",
      "<rasitaatrasr>\n",
      "<uainaitaur>\n",
      "<esasiuacusass>\n",
      "<amcsiiauas>\n",
      "<siiasao>\n",
      "<eppmysosu>\n",
      "<iaesasluanuuaus>\n",
      "<yslostanrar>\n",
      "\n",
      "Epoch:16\n",
      "<ortajnahua>\n",
      "<siiasan>\n",
      "<hatamuruau>\n",
      "<srmgprn>\n",
      "<ieprnyuruu>\n",
      "<iamesilauas>\n",
      "<uacsnaocursd>\n",
      "<erashtaausatr>\n",
      "<oubsssohtus>\n",
      "<arhtacspaso>\n",
      "<ilaisasiuanusass>\n",
      "<iysnpsuarsas>\n",
      "<oucsssrhuus>\n",
      "<alnaksaur>\n",
      "<ucsssodstr>\n",
      "\n",
      "Epoch:17\n",
      "<lgrrnyuru>\n",
      "<ashtaatrasr>\n",
      "<iagsashuaixsaur>\n",
      "<iprtamnaoua>\n",
      "<rtamnagtaur>\n",
      "<ivouctuurnuus>\n",
      "<iprtanoao>\n",
      "<acsoaocupra>\n",
      "<saiuouauuuss>\n",
      "<rriysosuuasshs>\n",
      "<rhtaasoasn>\n",
      "<giasanus>\n",
      "<arssngssr>\n",
      "<rsrngsso>\n",
      "<taimaitaur>\n",
      "\n",
      "Epoch:18\n",
      "<oubsssoguus>\n",
      "<eraritaausaus>\n",
      "<iprtanoapue>\n",
      "<esashuaausatr>\n",
      "<ieprnvuotu>\n",
      "<ketiiauar>\n",
      "<ysnrsuarsas>\n",
      "<amdsiiauar>\n",
      "<oubsssoiuus>\n",
      "<arhtaatraso>\n",
      "<iprtanoapue>\n",
      "<trlerso>\n",
      "<amcsiiauar>\n",
      "<uaimahsaur>\n",
      "<hasanurueu>\n",
      "\n",
      "Epoch:19\n",
      "<iahsasiuao>\n",
      "<rmdprmyuruu>\n",
      "<acsnaodupra>\n",
      "<iahsasluanuua>\n",
      "<oucsssoeuus>\n",
      "<imaisasl>\n",
      "<rrjysnrtuasses>\n",
      "<ashiasao>\n",
      "<idsiiauar>\n",
      "<alvotatuusr>\n",
      "<euacsnariurse>\n",
      "<iprtanoaoud>\n",
      "<ivouasuusou>\n",
      "<erarhtaatratr>\n",
      "<acsoaoeursc>\n",
      "\n",
      "Epoch:20\n",
      "<atanuruau>\n",
      "<cshiatanyt>\n",
      "<uacrnaodursc>\n",
      "<rmerrn>\n",
      "<giasanus>\n",
      "<erpmytosu>\n",
      "<snortaorar>\n",
      "<ucsssogtus>\n",
      "<hasalurue>\n",
      "<acrnaobuora>\n",
      "<argtaaspasn>\n",
      "<uotcsttrluus>\n",
      "<iahsasiuahusatr>\n",
      "<ieprnyuptu>\n",
      "<hiasaousvfu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_nlm(nlm, loss_fn,optimizer,epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can produce names, which cont-based  language can't produce (if we don't apply smoothing). Therefore - much more variabity  in generated names. Also we can indefinetely train the model until overfitting, but it generalizes well\n",
    "Also the quality of generated words is not as good and speed is of training is quite slow. I even had to decrease max length of word so training time would be reasonable\n",
    "It also needs much more epochs to produce okay names\n",
    "\n",
    "\n",
    "Parameters: size of hidden layer, max word size, dropout percentage, epochs, learning rate. \n",
    "- Max word size: after changing didn't change much, because words rarely reach >20 symbols. But the training time depends on it\n",
    "- size of hidden layer - increase of computation time, but quality and perplexity of names change a bit much  if increase (except if we set very small hidden layer). It has to big for proper generation\n",
    "- dropout - increase of computation time, but quality of names didn't change much (except if we set very high percentage\n",
    "- epochs & learning reate - epochs should be > 10 to generate proper names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    newline_idx = trn_ds.ch_to_idx['>']\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        start_char_idx = trn_ds.ch_to_idx['<']\n",
    "        indices = [start_char_idx]\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        \n",
    "        while predicted_char_idx != newline_idx and word_size != 50:\n",
    "            h_prev, y_pred = model(h_prev, x)\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float()\n",
    " \n",
    "            \n",
    "            predicted_char_idx = idx\n",
    "            \n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices\n",
    "\n",
    "def train_one_epoch(model, loss_fn, optimizer,dataset = trn_dl):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            h_prev, y_pred = model(h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def train(model, loss_fn, optimizer, dataset=trn_dl, epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch(model, loss_fn,optimizer, dataset)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN from class practise\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        combined = torch.cat([h_prev, x], dim = 1) # concatenate x and h\n",
    "        h = torch.tanh(self.dropout(self.i2h(combined)))\n",
    "        y = self.i2o(combined)\n",
    "        return h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<naiua>\n",
      "<hwacsoarts>\n",
      "<jatas>\n",
      "<qveus>\n",
      "<merus>\n",
      "<unrusaoras>\n",
      "<auluaauras>\n",
      "<uirasansus>\n",
      "<tursr>\n",
      "<snyus>\n",
      "<suaosaisas>\n",
      "<valusaopus>\n",
      "<auaos>\n",
      "<werusdirus>\n",
      "<torusanras>\n",
      "\n",
      "Epoch:2\n",
      "<aumuc>\n",
      "<sraop>\n",
      "<hlaudourus>\n",
      "<turlsuutus>\n",
      "<suainarras>\n",
      "<uaeusaurus>\n",
      "<aubls>\n",
      "<veruteorus>\n",
      "<trrusairas>\n",
      "<aurucnuras>\n",
      "<smrasapsus>\n",
      "<surosturus>\n",
      "<rusasoarur>\n",
      "<fuaeshcrus>\n",
      "<hbras>\n",
      "\n",
      "Epoch:3\n",
      "<pueus>\n",
      "<lirus>\n",
      "<sorusamras>\n",
      "<aumubauras>\n",
      "<snrasausus>\n",
      "<surortur>\n",
      "<sprusaurhs>\n",
      "<atruaauras>\n",
      "<snrasausus>\n",
      "<suros>\n",
      "<rpuus>\n",
      "<suaosaesis>\n",
      "<tbgusaurus>\n",
      "<aucnsturus>\n",
      "<llrus>\n",
      "\n",
      "Epoch:4\n",
      "<srrusauras>\n",
      "<aupucauras>\n",
      "<snrasassus>\n",
      "<suronturus>\n",
      "<puranraryc>\n",
      "<euacsicrus>\n",
      "<hcras>\n",
      "<pueusuurus>\n",
      "<zurostcrur>\n",
      "<scrosairua>\n",
      "<curoaterus>\n",
      "<burusaurus>\n",
      "<luruaeueussur>\n",
      "<ahroalnrus>\n",
      "<tdivsony>\n",
      "\n",
      "Epoch:5\n",
      "<spcpsotontotaurus>\n",
      "<aupucauraurus>\n",
      "<aublsiurus>\n",
      "<llrus>\n",
      "<sprusauras>\n",
      "<aurucauras>\n",
      "<snrasausus>\n",
      "<surnltur>\n",
      "<sprutaurhs>\n",
      "<aurubauras>\n",
      "<snrasassus>\n",
      "<surorturus>\n",
      "<purasaerur>\n",
      "<fuaasibrus>\n",
      "<hcras>\n",
      "\n",
      "Epoch:6\n",
      "<pueus>\n",
      "<llrus>\n",
      "<sorusaoras>\n",
      "<aurucaurhs>\n",
      "<smrasausus>\n",
      "<surortur>\n",
      "<sprusauras>\n",
      "<augucausaurus>\n",
      "<aucktiurus>\n",
      "<llrur>\n",
      "<srrusauros>\n",
      "<ailsap>\n",
      "<markusalrus>\n",
      "<oucushurus>\n",
      "<zurus>\n",
      "\n",
      "Epoch:7\n",
      "<anras>\n",
      "<aurucauras>\n",
      "<snecocaurus>\n",
      "<turhstusus>\n",
      "<sualpaurus>\n",
      "<tceusaurus>\n",
      "<aubotaurus>\n",
      "<llrrpturus>\n",
      "<anrar>\n",
      "<auoucauras>\n",
      "<snrasausus>\n",
      "<suroknur>\n",
      "<sprusasras>\n",
      "<aulucataatis>\n",
      "<hcsaryurus>\n",
      "\n",
      "Epoch:8\n",
      "<srlushurus>\n",
      "<tbroaaunus>\n",
      "<airoatesus>\n",
      "<sdrwrueeurus>\n",
      "<rpwuriuras>\n",
      "<anrasaueus>\n",
      "<arkunidlprus>\n",
      "<busopaurus>\n",
      "<ltruaaurus>\n",
      "<smsaekuaus>\n",
      "<gnaucaurus>\n",
      "<suifrun>\n",
      "<sprusauris>\n",
      "<aukucauras>\n",
      "<snrasaurus>\n",
      "\n",
      "Epoch:9\n",
      "<surokeurus>\n",
      "<pusaralrus>\n",
      "<euabsacrus>\n",
      "<hcrar>\n",
      "<pucusturus>\n",
      "<yunustcrus>\n",
      "<scrisalrua>\n",
      "<cupjbterus>\n",
      "<busumnosa>\n",
      "<srrusarris>\n",
      "<aukuaauras>\n",
      "<sniaocaurus>\n",
      "<sureshusus>\n",
      "<suanianatrusnus>\n",
      "<argunagras>\n",
      "\n",
      "Epoch:10\n",
      "<oudus>\n",
      "<llrur>\n",
      "<sprusatras>\n",
      "<alnsaeiurus>\n",
      "<gnbuciurus>\n",
      "<turetthuyurus>\n",
      "<jobgtaurun>\n",
      "<spamittras>\n",
      "<jvoveosrurus>\n",
      "<yurosserus>\n",
      "<sdritaurui>\n",
      "<cupibsirus>\n",
      "<burshergoyaurus>\n",
      "<irbisaurui>\n",
      "<sabritauous>\n",
      "\n",
      "Epoch:11\n",
      "<wstes>\n",
      "<sneosaurus>\n",
      "<tbnoaaurus>\n",
      "<agrobaurus>\n",
      "<scrus>\n",
      "<bususaurus>\n",
      "<ktruakraismur>\n",
      "<agopaorupkaue>\n",
      "<wructosin>\n",
      "<rnytisaurus>\n",
      "<euasmtcauros>\n",
      "<sancude>\n",
      "<ptgorsaurus>\n",
      "<sauruacurus>\n",
      "<sntaltoaaurus>\n",
      "\n",
      "Epoch:12\n",
      "<scrvsturus>\n",
      "<dsopysaurun>\n",
      "<kcgsdauros>\n",
      "<lbrashalros>\n",
      "<pudtsokmtos>\n",
      "<spotrcaurus>\n",
      "<snsainugus>\n",
      "<gnaucaurus>\n",
      "<ttgesausus>\n",
      "<suaosauras>\n",
      "<tcctoaurus>\n",
      "<auaavktnaurus>\n",
      "<rnwtsduras>\n",
      "<alrasaurus>\n",
      "<argunrasan>\n",
      "\n",
      "Epoch:13\n",
      "<pucuspurus>\n",
      "<yurusycrul>\n",
      "<scshsajrua>\n",
      "<cukkcdrrus>\n",
      "<bususaurus>\n",
      "<ltsuahuqus>\n",
      "<snsadculus>\n",
      "<gnbubaurus>\n",
      "<surisiurus>\n",
      "<suaimcgsaurus>\n",
      "<sraoltira>\n",
      "<antotdssaurus>\n",
      "<yunostasairus>\n",
      "<eubasccras>\n",
      "<hctapuurus>\n",
      "\n",
      "Epoch:14\n",
      "<splusiurus>\n",
      "<tbrnchodhurus>\n",
      "<laratesaurus>\n",
      "<tgotoilossuurus>\n",
      "<anraptorus>\n",
      "<curasaurur>\n",
      "<aptoteseurus>\n",
      "<kysnrstcrun>\n",
      "<scrisarrug>\n",
      "<cukoctdaurus>\n",
      "<surhsonuor>\n",
      "<suaipaltdsn>\n",
      "<ahsoanesgoc>\n",
      "<antusaurus>\n",
      "<rsiythosoerus>\n",
      "\n",
      "Epoch:15\n",
      "<scrisaerus>\n",
      "<curictrrus>\n",
      "<busoraurut>\n",
      "<ltsucauous>\n",
      "<smsahtuaus>\n",
      "<gnbucaurus>\n",
      "<turistusus>\n",
      "<suanncaurus>\n",
      "<ahsoaanrusauhus>\n",
      "<thsuulmgurus>\n",
      "<pusainaksaurus>\n",
      "<sraoismaasav>\n",
      "<ouassoraurus>\n",
      "<ltsuamodaurus>\n",
      "<aisoaburus>\n",
      "\n",
      "Epoch:16\n",
      "<tcrussurus>\n",
      "<durotturus>\n",
      "<iacrucaurus>\n",
      "<lbigkurus>\n",
      "<iujulnrsaurus>\n",
      "<sorusarapops>\n",
      "<euadsacrus>\n",
      "<gctchys>\n",
      "<busuraurut>\n",
      "<kusuaaucus>\n",
      "<smsaceukus>\n",
      "<gnauaaurus>\n",
      "<turbsiusus>\n",
      "<spbrocaurus>\n",
      "<ahrocaurus>\n",
      "\n",
      "Epoch:17\n",
      "<scivrodaurus>\n",
      "<rnytncuras>\n",
      "<aaucopturus>\n",
      "<laurndkurus>\n",
      "<btutrisaurus>\n",
      "<stbrocaurus>\n",
      "<agonaiitdamrus>\n",
      "<pucutoratopy>\n",
      "<lsrtaoraisus>\n",
      "<tccunasaurus>\n",
      "<antosaurus>\n",
      "<rurysjosomrus>\n",
      "<scrisalrua>\n",
      "<cuioatarus>\n",
      "<busupaurus>\n",
      "\n",
      "Epoch:18\n",
      "<lhurbroeaurus>\n",
      "<agrobggtoraxahus>\n",
      "<ckuroptor>\n",
      "<sauruboiaus>\n",
      "<smsahtuius>\n",
      "<hacreptotaurus>\n",
      "<rtgysaurur>\n",
      "<lcgtgnataa>\n",
      "<lagdpsras>\n",
      "<jvrudostor>\n",
      "<rqturottar>\n",
      "<amsaserasopa>\n",
      "<cublawdivlud>\n",
      "<tunhseurus>\n",
      "<suaopalta>\n",
      "\n",
      "Epoch:19\n",
      "<eubashbrus>\n",
      "<hcrar>\n",
      "<pucrsturus>\n",
      "<yumustcrus>\n",
      "<scoltaauras>\n",
      "<snecterys>\n",
      "<busoodopsuurus>\n",
      "<anraruargtaurus>\n",
      "<curocterus>\n",
      "<busuraurus>\n",
      "<lsptapaurus>\n",
      "<euaasabrus>\n",
      "<hcsaluurus>\n",
      "<spgusaurus>\n",
      "<tbmiaaurus>\n",
      "\n",
      "Epoch:20\n",
      "<ahopaopr>\n",
      "<hcrbitaurus>\n",
      "<llrupvurus>\n",
      "<anrastrrus>\n",
      "<burasaurus>\n",
      "<antotashurus>\n",
      "<jwtnosmnrus>\n",
      "<scsisaurus>\n",
      "<cugicria>\n",
      "<oucutopestt>\n",
      "<srrusaurus>\n",
      "<aukucauras>\n",
      "<smnasamvosaurus>\n",
      "<rsgzsosaurusaoras>\n",
      "<tccuraurus>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "rnnn = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "optimizer = optim.SGD(rnnn.parameters(), lr=1e-2)\n",
    "train(rnnn, loss_fn, optimizer, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llsonzurus>\n"
     ]
    }
   ],
   "source": [
    "print_sample((sample(rnnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key differences:\n",
    "- RNN trains faster because the input layer is not as big\n",
    "- RNN performs better, words are more real\n",
    "- it doesn't remember all prev symbols in constrast with NLP\n",
    "- It is similar to the NLM, just that after encoding the current input word we feed the resulting representation into a LSTM, which then outputs a vector. Then we use the decoder to convert this output vector into a vector of probability values\n",
    "\n",
    "Parameters:\n",
    "- NLM: 5 parameters size of hidden layer, max word size, dropout percentage, epochs, learning rate.\n",
    "- Recurrent: also 5 parameters, but max_word_size is not such important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.4.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sampe prob for RNN model\n",
    "def sample_probablity(name,model):\n",
    "    model.eval()\n",
    "    total_probab = 1.0\n",
    "    h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "\n",
    "    x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "\n",
    "    start_char_idx = trn_ds.ch_to_idx['<']\n",
    "\n",
    "    indices = [start_char_idx]\n",
    "\n",
    "    x[0, start_char_idx] = 1\n",
    "    predicted_char_idx = start_char_idx\n",
    "    # One-code input name\n",
    "    x = torch.zeros([len(name), trn_ds.vocab_size], dtype=torch.float)\n",
    "    for idx in range(0,len(name)):\n",
    "        x[idx][trn_ds.ch_to_idx[name[idx]]] = 1\n",
    "    # Calculate prob (just by taking maximum in y_softmax_scores)\n",
    "    for idx in range(0,len(name)-1):\n",
    "        h_prev, y_pred = model(h_prev, x[idx][None])\n",
    "        y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "        total_probab*=y_softmax_scores.max().item()\n",
    "    return total_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sample prob for nlm model\n",
    "def sample_probablity_nlm(name,model):\n",
    "    model.eval()\n",
    "    total_probab = 1.0\n",
    "    h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "\n",
    "    x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "\n",
    "    start_char_idx = trn_ds.ch_to_idx['<']\n",
    "\n",
    "    indices = [start_char_idx]\n",
    "    \n",
    "    x[0, start_char_idx] = 1\n",
    "    predicted_char_idx = start_char_idx\n",
    "    # One-code input name\n",
    "    x = torch.zeros([len(name), trn_ds.vocab_size], dtype=torch.float)\n",
    "    for idx in range(0,len(name)):\n",
    "        x[idx][trn_ds.ch_to_idx[name[idx]]] = 1\n",
    "    # Calculate prob (just by taking maximum in y_softmax_scores)\n",
    "    for idx in range(0,len(name)-1):\n",
    "        \n",
    "        xx = torch.cat([x[:idx+1].view(-1), torch.zeros( 870-x[:idx+1].view(-1).shape[0] )], 0)\n",
    "        h_prev, y_pred = model(h_prev, xx[None])\n",
    "        \n",
    "        y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "        total_probab*=y_softmax_scores.max().item()\n",
    "    return total_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(name,lm):\n",
    "    \"\"\"\n",
    "    lm = count-based LM,\n",
    "    or\n",
    "    count-based with add-one smoothing LM\n",
    "    or\n",
    "    neural LM\n",
    "    or\n",
    "    RNN-LM\n",
    "    \"\"\"\n",
    "    total_prob = 1.0\n",
    "    if lm == 'count-based LM':\n",
    "        for idx in range(0,len(name)-1):\n",
    "            total_prob *= cprob[name[idx]].prob(name[idx+1])\n",
    "    if lm == 'count-based with add-one smoothing LM':\n",
    "        for idx in range(0,len(name)-1):\n",
    "            total_prob *= cprob_laplace[name[idx]].prob(name[idx+1])\n",
    "    if lm == 'neural LM':\n",
    "        total_prob = sample_probablity_nlm(name,nlm)\n",
    "    if lm == 'RNN-LM':\n",
    "        total_prob = sample_probablity(name,rnnn)\n",
    "\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '<aukucauras>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.464050746210654e-11"
      ]
     },
     "execution_count": 1304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob(name,'count-based LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.804742604109511e-11"
      ]
     },
     "execution_count": 1305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob(name,'count-based with add-one smoothing LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4510260682161353e-09"
      ]
     },
     "execution_count": 1306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob(name,'neural LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.915068917332468e-06"
      ]
     },
     "execution_count": 1307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob(name,'RNN-LM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "train, test = train_test_split(trn_ds.lines,test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds_train = DinosDataset()\n",
    "trn_ds_train.lines = train\n",
    "trn_dl_train =  DataLoader(trn_ds_train, shuffle=True)\n",
    "trn_ds_test = DinosDataset()\n",
    "trn_ds_test.lines = test\n",
    "trn_dl_test =  DataLoader(trn_ds_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count-based LM \n",
    "names = train\n",
    "chars = [char  for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "l = sum([freq[char] for char in freq])\n",
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add-one smoothing LM\n",
    "vocabulary = nltk.FreqDist(flatten(names))\n",
    "cprob_laplace = nltk.ConditionalProbDist(cfreq, nltk.LaplaceProbDist, bins=len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<oaluauo>\n",
      "<sm>\n",
      "<alypubuvusn>\n",
      "<rlzsmrsuanoanuauo>\n",
      "<uilauas>\n",
      "<sngrro>\n",
      "<astsnesso>\n",
      "<asnaoeurra>\n",
      "<lerrm>\n",
      "<rrjysmosuarscs>\n",
      "\n",
      "Epoch:2\n",
      "<ashtaatsasr>\n",
      "<lcsiiasao>\n",
      "<errnytpsu>\n",
      "<atalupuatuusr>\n",
      "<csiiatao>\n",
      "<atalusuauu>\n",
      "<almagtauo>\n",
      "<kahsasi>\n",
      "<xoucstsohuus>\n",
      "<rlysnrtuassas>\n",
      "\n",
      "Epoch:3\n",
      "<asitadtsaus>\n",
      "<siiatao>\n",
      "<rlysnrtuarsas>\n",
      "<ilahsasmyai>\n",
      "<hasalusuduu>\n",
      "<uainaltaur>\n",
      "<uainaiuaur>\n",
      "<oucstsoiuuu>\n",
      "<hasanusueu>\n",
      "<iahrasiuai>\n",
      "\n",
      "Epoch:4\n",
      "<aluotcsuusr>\n",
      "<asssnestr>\n",
      "<uainahuaur>\n",
      "<arhtaasparo>\n",
      "<snprtanoap>\n",
      "<gjatalurueu>\n",
      "<rlysmrsuassds>\n",
      "<imaisatn>\n",
      "<ahsashuacusasr>\n",
      "<ataluouauuysr>\n",
      "\n",
      "Epoch:5\n",
      "<amdsiiauas>\n",
      "<strngrsr>\n",
      "<iamesilauas>\n",
      "<imaisatn>\n",
      "<fiasanusyeu>\n",
      "<tsngrrn>\n",
      "<iepplysosu>\n",
      "<rlysnrsuarses>\n",
      "<acroaoeurra>\n",
      "<acsnaoduora>\n",
      "\n",
      "Epoch:6\n",
      "<ancsiiauas>\n",
      "<uainahuaus>\n",
      "<yslpsuaosas>\n",
      "<smpstanpapua>\n",
      "<iwouasuurox>\n",
      "<uacsnaoeuora>\n",
      "<ialesimauas>\n",
      "<rlaneslnaucs>\n",
      "<ipstanoaoua>\n",
      "<rnandskmauas>\n",
      "\n",
      "Epoch:7\n",
      "<iahrasiuaiusaus>\n",
      "<smrstaorar>\n",
      "<ivotcsuurou>\n",
      "<ucsssogstr>\n",
      "<astsngstr>\n",
      "<rriysnsuuasuos>\n",
      "<asmanetmoauis>\n",
      "<sasiuaausatr>\n",
      "<uaasnaoaurse>\n",
      "<acrnaoauora>\n",
      "\n",
      "Epoch:8\n",
      "<etabsoaoeursa>\n",
      "<iagsasluamusaur>\n",
      "<hlataoys>\n",
      "<rlyrnpsuarscr>\n",
      "<hatanuszeu>\n",
      "<rarhtaatratp>\n",
      "<andsiiauar>\n",
      "<tslgrso>\n",
      "<erariuaausasr>\n",
      "<saiuouatuyso>\n",
      "\n",
      "Epoch:9\n",
      "<rtsngrso>\n",
      "<rlysnrtuassfs>\n",
      "<uacroaoeursc>\n",
      "<arssndstr>\n",
      "<raritaausasp>\n",
      "<rnandsknauds>\n",
      "<igrrn>\n",
      "<assrngstr>\n",
      "<rmeprn>\n",
      "<armaodunra>\n",
      "\n",
      "Epoch:10\n",
      "<allahsaup>\n",
      "<ortanoanuaus>\n",
      "<rmeprnyurtu>\n",
      "<snprtanoar>\n",
      "<idsiiauar>\n",
      "<allaisatouap>\n",
      "<rtrnessp>\n",
      "<esasiuaausaus>\n",
      "<rsrohssr>\n",
      "<iysnrsuarsas>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# neural LM\n",
    "nlm = NLM(trn_ds.vocab_size*30, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(nlm.parameters(), lr=1e-2)\n",
    "train_nlm(nlm, loss_fn,optimizer,epochs = 10,dataset = trn_dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<maiuauo>\n",
      "<actoaofsosauaoyutau>\n",
      "<uniustysus>\n",
      "<waotatuc>\n",
      "<huaes<arts>\n",
      "<iaras>\n",
      "<pueus>\n",
      "<merus>\n",
      "<tnrusaorgs>\n",
      "<aulucctras>\n",
      "\n",
      "Epoch:2\n",
      "<tinasausus>\n",
      "<turus>\n",
      "<sayrss>\n",
      "<uaroceugus>\n",
      "<aaurasaurug>\n",
      "<antusdusus>\n",
      "<stoysosrua>\n",
      "<laluh>\n",
      "<guabslaros>\n",
      "<hasas>\n",
      "\n",
      "Epoch:3\n",
      "<pugus>\n",
      "<lirus>\n",
      "<tauruciueusaurun>\n",
      "<saarns>\n",
      "<haueus>\n",
      "<verusoisus>\n",
      "<trrusauros>\n",
      "<auouaaurasouroc>\n",
      "<antssaus>\n",
      "<lauruv>\n",
      "\n",
      "Epoch:4\n",
      "<liuras>\n",
      "<aaudus>\n",
      "<agroaterusaulus>\n",
      "<veruseurus>\n",
      "<snrusaurns>\n",
      "<auruaauras>\n",
      "<tnrasaisus>\n",
      "<sourisuusus>\n",
      "<suaosauros>\n",
      "<uaerucuisusoulus>\n",
      "\n",
      "Epoch:5\n",
      "<vaurus>\n",
      "<siusus>\n",
      "<srbrocaueus>\n",
      "<agrobanrus>\n",
      "<tcrusvurus>\n",
      "<dtrov>\n",
      "<lurucauius>\n",
      "<saucessarus>\n",
      "<hcsas>\n",
      "<pugusturus>\n",
      "\n",
      "Epoch:6\n",
      "<zlrrusaoras>\n",
      "<ainsantuaus>\n",
      "<gnbuchurup>\n",
      "<turistutus>\n",
      "<suaorasros>\n",
      "<tacrualurus>\n",
      "<tarsusausus>\n",
      "<strysiuruc>\n",
      "<lcctaorybdosaurus>\n",
      "<aucottun>\n",
      "\n",
      "Epoch:7\n",
      "<tmurus>\n",
      "<zurus>\n",
      "<anras>\n",
      "<auruaauras>\n",
      "<snrasaisuuaus>\n",
      "<llrur>\n",
      "<srrusauras>\n",
      "<auquaauras>\n",
      "<snrasausus>\n",
      "<smuros>\n",
      "\n",
      "Epoch:8\n",
      "<kyurus>\n",
      "<anrar>\n",
      "<auruaauras>\n",
      "<tatcumous>\n",
      "<burusdus>\n",
      "<zurustcrus>\n",
      "<saurucauris>\n",
      "<samducaurus>\n",
      "<turastutus>\n",
      "<suaisaisaurus>\n",
      "\n",
      "Epoch:9\n",
      "<sabros>\n",
      "<hcrar>\n",
      "<pterusaurus>\n",
      "<snrutahsaurus>\n",
      "<taeroaotrusuulut>\n",
      "<uaurusaurus>\n",
      "<liusastarus>\n",
      "<esciusaurus>\n",
      "<ancrus>\n",
      "<blurus>\n",
      "\n",
      "Epoch:10\n",
      "<sayrosoueus>\n",
      "<eocrosaarua>\n",
      "<cinrcsausus>\n",
      "<surrr>\n",
      "<sayrosouius>\n",
      "<ejcrnsaurud>\n",
      "<curlctrrus>\n",
      "<bususaurus>\n",
      "<ltruahubus>\n",
      "<smsai>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN_LM \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "rnnn = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "optimizer = optim.SGD(rnnn.parameters(), lr=1e-2)\n",
    "train(rnnn, loss_fn, optimizer, epochs = 10,dataset = trn_dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_count_based = 0 \n",
    "entropy_smoothing_LM =0 \n",
    "entropy_nlm = 0\n",
    "entropy_rnn = 0\n",
    "for name in test:\n",
    "    entropy_count_based+=get_prob(name,'count-based LM')\n",
    "    entropy_smoothing_LM+=get_prob(name,'count-based with add-one smoothing LM')\n",
    "    entropy_nlm+=get_prob(name,'neural LM')\n",
    "    entropy_rnn+=get_prob(name,'RNN-LM')\n",
    "entropy_count_based /=len(test)\n",
    "entropy_smoothing_LM /=len(test) \n",
    "entropy_nlm /=len(test)\n",
    "entropy_rnn /=len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for count-based LM  1.0000003375260127\n",
      "Perplexity for count-based LM with smoothing  1.0000003003758293\n",
      "Perplexity for NLM 1.0000012759394068\n",
      "Perplexity for RNN-LM  1.0006227088755617\n"
     ]
    }
   ],
   "source": [
    "# Perplexity:\n",
    "print(\"Perplexity for count-based LM \", 2**entropy_count_based)\n",
    "print(\"Perplexity for count-based LM with smoothing \", 2**entropy_smoothing_LM)\n",
    "print(\"Perplexity for NLM\", 2**entropy_nlm)\n",
    "print(\"Perplexity for RNN-LM \", 2**entropy_rnn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can clearly see, that RNN has outperformed all models. The reason is that it is the state-of-the-model, which:\n",
    "1) can generate more various names\n",
    "2) while generating this names, it still generalizes well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
